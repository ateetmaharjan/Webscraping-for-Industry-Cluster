{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries (make sure that they are installed on your environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# Python library for pulling data out of HTML and XML files.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "# selenium sends the standard Python commands to different browsers, despite variation in their browser's design.\n",
    "# Python APIs empower us to connect with the browser through Selenium.\n",
    "# module provides all the WebDriver implementations.\n",
    "\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "import json  # import if you are converting to json file as your output\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages using pip in terminal or !pip in jupyternotebook cell or install using following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Successfully installed\n"
    }
   ],
   "source": [
    "package = \"selenium\"\n",
    "try:\n",
    "    __import__package\n",
    "    print(\"imported\")\n",
    "except:\n",
    "    os.system(\"pip install \" + package)\n",
    "    print(\"Successfully installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In browser, right click, inspect element, go to network and check for data, and check for elements to understand how the data table looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all required links (from where you want to scrape data)\n",
    "loopy = [\n",
    "\"https://data.bls.gov/cew/apps/table_maker/v4/table_maker.htm#type=6&year=2015&qtr=A&own=5&area=39000&supp=0\",\n",
    "\"https://data.bls.gov/cew/apps/table_maker/v4/table_maker.htm#type=6&year=2018&qtr=A&own=5&area=39000&supp=0\",\n",
    "\"https://data.bls.gov/cew/apps/table_maker/v4/table_maker.htm#type=6&year=2018&qtr=A&own=5&area=39001&supp=0\",\n",
    "\"https://data.bls.gov/cew/apps/table_maker/v4/table_maker.htm#type=6&year=2015&qtr=A&own=5&area=39001&supp=0\",\n",
    "\"https://data.bls.gov/cew/apps/table_maker/v4/table_maker.htm#type=6&year=2018&qtr=A&own=5&area=US000&supp=0\",\n",
    "\"https://data.bls.gov/cew/apps/table_maker/v4/table_maker.htm#type=6&year=2015&qtr=A&own=5&area=US000&supp=0\"\n",
    "]\n",
    "\n",
    "# CREATE A LOOP TO INTERATE THROUGH EACH ELEMENT IN loopy\n",
    "\n",
    "for stateYear in loopy:\n",
    "    baseUrl = stateYear\n",
    "    baseYear = baseUrl[73:77]  # extract year string\n",
    "    # extract code string (US000 for US, 39000 for Ohio, 39001 for Adam's County)\n",
    "    baseCode = baseUrl[95:100]\n",
    "\n",
    "    # To link browser and trigger browser to load the data, it requires chromedriver or other browser driver\n",
    "    # Download chromeDriver from https://chromedriver.storage.googleapis.com/index.html?path=81.0.4044.138/\n",
    "    # browser = webdriver.Chrome(executable_path=r\"./chromedriver.exe\")  # for windows\n",
    "    browser = webdriver.Chrome(\n",
    "        executable_path=r\"./chromedriver\")  # for mac, locate file path\n",
    "\n",
    "    try:\n",
    "        browser.get(baseUrl)\n",
    "        page = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "        tbody = page.find(id='cewTableBody')\n",
    "        # create headings to avoid hassle of getting the headings from web\n",
    "        headings = ['NAICS_Sector', 'Annual_Establishments', 'Annual_Average_Employment', 'Total_Annual_Wages',\n",
    "        'Annual_Average_Weekly_Wage','Annual_Wages_per_Employee', 'Annual_Average_Employment_Location_Quotient',\n",
    "        'Total_Annual_Wages_Location_Quotient']\n",
    "\n",
    "        table_data = []\n",
    "        for tr in tbody.find_all(\"tr\"):  # find all table row from tbody\n",
    "            t_row = {}\n",
    "            for td, th in zip(tr.find_all(\"td\"), headings):\n",
    "                t_row[th] = td.text.replace('\\n', '').replace('$', '').strip() # replace $ sign, and new line with no space\n",
    "            table_data.append(t_row)\n",
    "        # data is already stored in table_data\n",
    "\n",
    "\n",
    "        # convert list to dataframe\n",
    "        table_data_df = pd.DataFrame(table_data)\n",
    "        # make new dataframe with NAICS_sector column split where there is space\n",
    "        # we want the code and name to be separate\n",
    "        new = table_data_df[\"NAICS_Sector\"].str.split(\" \", 2, expand=True)\n",
    "        # making separate columns from dataframe separated by space and combining\n",
    "        table_data_df[\"NAICS_Sector_Code\"] = new[0]+\" \"+new[1]\n",
    "        # making separate last industry column from new data frame\n",
    "        table_data_df[\"NAICS_Sector_Industry\"] = new[2]\n",
    "        # rearrange columns and overwriting on same dataframe\n",
    "        table_data_df = table_data_df[[\n",
    "            'NAICS_Sector_Code', 'NAICS_Sector_Industry', 'NAICS_Sector', 'Annual_Establishments',\n",
    "            'Annual_Average_Employment', 'Total_Annual_Wages', 'Annual_Average_Weekly_Wage',\n",
    "            'Annual_Wages_per_Employee', 'Annual_Average_Employment_Location_Quotient',\n",
    "            'Total_Annual_Wages_Location_Quotient']]\n",
    "            \n",
    "        # table_data_df.drop(columns=[\"NAICS_Sector\"], inplace=True)         # to drop the old columns\n",
    "\n",
    "        fileName = baseCode+\"_\"+baseYear+\".csv\"         # create a file name with area code and year in csv\n",
    "        path=r'./Web_scraped_raw_data'                  # create a folder path to store csv files\n",
    "\n",
    "        # table_data_df.to_csv(path,fileName,index=False)  # write as csv file\n",
    "        table_data_df.to_csv(os.path.join(path,fileName),index=False) # write as csv file to a specific folder\n",
    "\n",
    "\n",
    "        # fileName = baseCode+\"_\"+baseYear+\".json\"      # to output as json file\n",
    "        # with open(fileName, 'w') as fp:\n",
    "        #     json.dump(table_data_df, fp)\n",
    "\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert jupyter notebook into html, run this code on terminal\n",
    "\n",
    "# pip install nbconvert\n",
    "# jupyter nbconvert --to html webscraping.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}